---
title: "Core Sims"
author: "Katie Lotterhos"
date: "December 13, 2015"
output: html_document
---

In this document, I will make the dataframe for the core Nemo Simulations.

### Fitness function
We are using a Gaussian (quadratic) function to describe stabilizing selection on the phenotype ($z$) of individual ($i$) in population ($k$) with phenotypic optimum $\Theta$, and selection variance $\omega_k^2$:

$$ W_{z_{i,k}}= 1 - \frac{(z_{i,k}-\Theta_k)^2}{\omega_k^2}$$

As an R function:

```{r}

w.zik <- function(zik, theta, omega.sq){
  1 - (zik - theta)^2/omega.sq
}
```

For the 2-patch model, we assume that each patch has an optimum of +1 and -1.  We assume that for a given number of loci that affect the trait(`ntot`) at the lowest level of redundancy, their effect sizes (`alpha`) are $1/(2*ntot)$.  For this, we can use the supplemental equation from Yeaman 2015 Am Nat to calculate the critical migration rate $m_c$.  At $m>m_c$, alleles are prone to swamping by migration.

In our SLiM simulation, we can simulate up to 2000 SNPs of equal effect sites.

```{r}
theta.1 <- 1
theta.2 <- -1
omega.sq <- 25
ntot <- c(1, 2, 10,20,50,100,500)
alpha <- 1/(2*ntot)
# alpha is effect size input to SliM

m_crit <- function(alpha, theta.1, theta.2, omega.sq, N){
  w1bb <- 1#w.zik(4*alpha,theta.1, omega.sq)/
    #w.zik(4*alpha, theta.1, omega.sq)
  w1Bb <- w.zik(2*alpha,theta.1, omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  w1BB <- w.zik(0, theta.1,omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  
  w2bb <- w.zik(4*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2Bb <- w.zik(2*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2BB <- 1#w.zik(0, theta.2,omega.sq)/
    #w.zik(0, theta.2, omega.sq)
  
  w1bb;w1Bb; w1BB
  w2bb;w2Bb; w2BB
  return(1/(w1Bb/(w1Bb-w1BB*(1+(1/(4*N))))-w2Bb/(w2BB*(1+(1/(4*N)))-w2Bb))
  )
}

N=1000
mc <- m_crit(alpha, theta.1, theta.2, omega.sq, N)
cbind(ntot, alpha, mc=round(mc,5), lowmc=0.0100*round(mc,5), highmc=100*round(mc,5))
```

Note that for the smallest effect size, alleles will always be prone to swamping by migration.

### Levels 

Now, we want to extend this base set to cover redundancy (1.5 ntot, 2 ntot), mutation rate, Ne (1000, 5000), environmental variance (0,2,4), and 9 migration rates (0.01mc to 100mc) spanning the critical threshold, standing variation vs. new mutation, clustered vs. unclustered.   This gives a maximum of :
```{r}
tot <- 
length(ntot)* #levels polygenicity
  3* #levels redundancy 1, 2, 4
  2* #levels Ne, 1000, 10000
  3* #levels envi var, 
  7* #levels migration, log scale, mc/1000, mc/100, mc/10, and mc, 5mc, 10mc, 50mc
  4* #levels, sample runif from Ne*mu=0.0001, 0.001, 0.01, 0.1
  2* # levels recombination
  #2 # standing variation vs. new mutation
  3*  # selection strength omega.sq, sample runif from 5, 25, 50
  5 # replicates per level

tot
(tot*3) # total number of minutes
min_serv = (tot*3)/70 # total number of minutes per server (60 servers)
min_serv/(60*24) # days needed
```

Note that some levels will not be possible (e.g. $m_c=0$ for ntot>400 and $m_c$>1).   

Note also that the larger Ne simulations will be a subset of the total number.

Eventually we will add more biological realism to the simulations.

### Mutation and Ne levels
For mutation, let's assume we want to at least capture $N_e\mu$=1e-04, which would approximate humans.  
This gives a per-base pair mutation rate $\mu_1$=1e-07 when $N_e_1=1000$ or $\mu_2$=1e-08 when $N_e_2=10000$. 

```{r}
Ne <- c(1000, 10000)
Ne_mu <- c(1e-04, 1e-03, 1e-02, 1e-01)

Ne_table <- expand.grid(Ne=Ne, Ne_mu=Ne_mu)
Ne_table$mu <- Ne_table$Ne_mu/Ne_table$Ne
Ne_table

# Let's take out some of the options for large Ne
Ne_table <- Ne_table[-c(4, 8),]

Ne_table
# 7 levels of Ne_mu
```

### Recombination


in humans 50,000 bp would correspond to 0.05 cM

proximate bases with $N_er=0.01$ corresponds to every 10,000 bases in humans

proximate bases with $N_er=0.001$ corresponds to every 1,000 bases in humans

proximate bases with $N_er=0.1$ corresponds to every 100,000 bases in humans

"CgsSNP, the average numbers of SNPs per 10 kb was 8.33, 8.44, and 8.09 in the human genome, in intergenic regions, and in genic regions, respectively."

Each block in the SliM simulation is 1000 bases long with the QTN in the center. A base recombination rate of $N_er=0.01$) gives
a resolution of 0.001 cM between proximate bases. This mimics the case where the SNPs were collected across a larger
genetic map than what was simulated. At this resolution, a chromosome would be 50,000 bases long (e.g. 50,000 bases / (r=1e-05) * 100 = 50 cM).


We will vary the recombination rate to be an order of magnitude larger and smaller than that:

```{r}

Ne_r_table <- rbind(cbind(Ne_table, Ne_r=0.01),
                    cbind(Ne_table, Ne_r=0.001),
                    cbind(Ne_table, Ne_r=0.1)
)

Ne_r_table$r <- Ne_r_table$Ne_r/ Ne_r_table$Ne

Ne_r_table
```


*From Thornton et al. 2013*
(Using a simulator similar to SLiM for a 100kB region)
We simulated a population of N=20,000 diploids with a neutral mutation rate of u=0.00125 per gamete per generation, and a recombination rate of r=0.00125 per diploid per generation. These values correspond to the scaled parameters $\theta=4N\mu=100$ and $\rho=4Nr=100$, and thus correspond to a ‘‘typical’’ 100 kilobase region of the human genome. The mutation rate to causative (deleterious) mutations was $\mu_d=0.1\mu$ per gamete per generation. 

(Here, the logic is 0.00125/100,000 bases ~ $10^{-8}$)

**From a<**
The per-nucleotide mutation rate $u$ and recombination
rate $r$ were assumed to be equal to $10^{-7}$, implying values of $N_eu=N_er=10^{-4}$, which are appropriate for human populations (Li and Sadler 1991; Kong et al. 2002). Thus, because we used Ne = 1000 in the simulations and effective sizes for human populations are an order ofmagnitude larger (see, e.g., Charlesworth 2009), we increased the mutation and recombi- nation rates by an order ofmagnitude to simulate the genetic variation corresponding to a population that is 10 times larger. The scaled recombination rate is consistent with an average value of 1 cM/Mb in the genome.

A constant unstructured population of size N = Ne = 1000 individuals was run for 10,000 generations. This burn-in period ensured that allele frequencies were close to mutation-selection equilibrium. In the final burn-in generation, the population was expanded to 10,000 individuals to simulate a frequency distribution of genetic variants corresponding to an unscaled population size that was 10 times larger


### Some core levels

```{r}
envi_var <- c(0, 1, 2)
sel.strength <- c(5,25,50)
mc_mult <- c(1/1000, 1/100, 1/10, 1, 2, 5, 10, 20, 100)
  
params2 <- expand.grid(envi_var=envi_var, 
            sel.strength=sel.strength,
             mc_mult= mc_mult)

dim(params2)
```

For each of the above levels, we have multiple levels of allele effect sizes.

### Make qtl input files for each nloc (3 reps)

```{r}

ntot

alpha
redundancy <- c(1, 2, 4, 10, 20, 40)

nqtl_levels <- c()
alpha_levels <- c()
for(i in seq_along(alpha)){
  nqtl_levels <- c(nqtl_levels, ntot[i]*redundancy)
  alpha_levels <- c(alpha_levels, rep(alpha[i],length(redundancy)))
}

(qtlbase <- data.frame(nqtl_levels, alpha_levels, redundancy=redundancy))

(qtlbase <- qtlbase[-which(qtlbase$nqtl_levels > 2000),]) # only have 2000 sites in core sim

```


### Expand grid to include levels of migration

```{r}
# size of loop
(nsims <- nrow(qtlbase) *
nrow(Ne_r_table) *
nrow(params2))
# max number of simulations

i = 0
k = 0
for (a in 1:nrow(qtlbase)){
  for (b in 1:nrow(Ne_r_table)){
    for (c in 1:nrow(params2)){
      k = k+ 1
      if(k%%1000==0){print(c(k, "of," ,nsims))}
      mc <- m_crit(qtlbase$alpha_levels[a], theta.1, theta.2, params2$sel.strength[c], Ne_r_table$Ne[b])
      m <- mc*params2$mc_mult[c]
      if(m > 0 & m < 0.5){
        i = i+1
        if(i==1){
          final_df <- data.frame(qtlbase[a,], Ne_r_table[b,], params2[c,], mc=mc, m=m)
        }else{
          final_df[i,] <- cbind(qtlbase[a,], Ne_r_table[b,], params2[c,], mc=mc, m=m)
        }
      }# end if m
    }# end c
  }# end b
}# end a

dim(final_df)

final_dfreps <- rbind(cbind(final_df, rep=1),
                      cbind(final_df, rep=2),
                      cbind(final_df, rep=3),
                      cbind(final_df, rep=4),
                      cbind(final_df, rep=5)
)

dim(final_dfreps)

seed_start <- 100000
final_dfreps$seed <- seed_start:(seed_start+nrow(final_dfreps)-1)
```
```{r}
write.table(final_dfreps, "/Users/katie/Desktop/Repos/TestTheTests/CoreSim_QTL/src/simparams.txt", row.names=FALSE, col.names = TRUE)
```








```{r old}
i=0;params <- c()
for (a in 1:nrow(qtlmap)){  # each row is a replicate
  for (b in seq_along(Ne)){ # Ne and mu have the same length
    for (d in seq_along(omega.sq_levels)){
    mc<- m_crit(qtlmap$alpha_qtlmap[a], theta.1, theta.2, omega.sq_levels[d], Ne[b]) 
    if (mc<=0){
      mc1 <- NULL
    }else{mc1<-mc}
    mig2 <- c(mig_levels, mc1)
    (mig2 <- sort(mig2))
   
        out <- expand.grid(alpha=qtlmap$alpha_qtlmap[a], ntot=qtlmap$nloc_qtlmap[a], 
                           Ne=Ne[b], mu=mu[b],
                           mc=mc, 
                           m=mig2, 
                           redundancy = qtlmap$redun[a],
                           envi_var=envi_var,  rep=qtlmap$rep[a],
                           mapseed=qtlmap$qtlmapseed[a],
                           omega_sq=omega.sq_levels[d],
                           qtlmapfile = qtlmap$filename_qtlmap[a],
                           demog="IM2")
        out <- data.frame(out)
      if (i==1){
      params <- out
      }else{
        params <- rbind(params, out)
      }#end ifelse
      }#end if m
    }# end loop c
    }# end loop d
  }  # end loop b
}# end loop a 
# this only takes a minute or two

dim(params)
params$resolution <- params$mu*100 # set recombination equal to mutation
params$NeMu <- params$Ne*params$mu
params$gens_t1 <- "1, 1000, 10000, 50000"
params$gens_t0 <- 50000
params$numneut <- 8400
params$npops <- 2
params$typeselection <- "stnvar"
levels(factor(params$NeMu))
tail(head(params,180), 10)

### Get a prelim set

levels(factor(params$alpha))

### Preliminary set for large effect
set1 <- params$alpha==0.25 & params$Ne==1000 & params$redundancy==2 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set1)
params[set1,]

### Preliminary set for small effect
set2 <- params$alpha==0.005 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set2)
params[set2,]

set3 <- params$alpha==0.025 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set3)
params[set3,]

par(mfrow=c(3,1), mar=c(4,1,2,1))
  hist(log(params$m[set1],10), breaks=-5:1, main = "large effect mini set")
  arrows(log(params$mc[set1],10), 10, log(params$mc[set1],10), 0, lwd=3)
  
  hist(log(params$m[set3],10), breaks=-5:1, main = "medium effect mini set")
  arrows(log(params$mc[set3],10), 10, log(params$mc[set3],10), 0, lwd=3)

  hist(log(params$m[set2],10), breaks=-5:1, main = "small effect mini set")
  arrows(log(params$mc[set2],10), 10, log(params$mc[set2],10), 0, lwd=3)
```

### For now use a reduced set of parameters

```{r}
params2 <- params
params <- rbind(params[set1,], params[set3,], params[set2,])
#params<- params[params$m<1e-02 & params$m>2e-03,]
```

### Load in Allele Freq Spectrum table


```{r}
afbins <- read.table("createStnVar/TableAlleleFreqBins.txt", header=TRUE)
levels(factor(params$NeMu))

get_p <- function(NeMu){
  if(NeMu==0.0001)(p <- afbins$Neu.0.0001)
  if(NeMu==0.001)(p<- afbins$Neu.0.001)
  if(NeMu==0.01)(p<- afbins$Neu.0.01)
  if(NeMu>0.01)(p<- afbins$Neu.0.1)
  a <- sample(afbins$levs, size=1, prob=p)
  pout <- runif(1,min=a-0.049, max=a)
  if(pout<0.001){pout=0.001} # this is something to note
  return(sample(c(pout, 1-pout),1))
}

get_beta <- function(Ne, m, u, npops){
  p <- get_p(round(Ne*u,4))
  a <- 4*Ne*m*p
  b <- 4*Ne*m*(1-p)
  return(rbeta(npops, shape1=a, shape2=b))
}

nemo_ntrl_init_freq <- function(Ne, m, u, npops, nloci){
  bob <- c()
  if(npops>1){
      freqmat <- replicate(n=(nloci),
                           get_beta(Ne,m,u,npops))
    for (i in 1:npops){
      bob[i] <- paste("{",paste(sprintf("%f",
                freqmat[i,]), collapse=", "),"}")
    }
  }
  if(npops==1){
    freqmat <- replicate(n=nloci, get_p(round(Ne*u,4)))
    bob <- paste("{",paste(sprintf("%f", freqmat), collapse=", "),"}")
  }
  return(paste("{", paste(bob, collapse="") ,"}"))
}

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-07,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.0001,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

nemo_ntrl_init_freq(1000,m=1e-04,1e-05,2, 10)
nemo_ntrl_init_freq(1000,m=1e-04,1e-05,1, 10)
# #
# b <- 4Nem*(1-p)
# rbeta(2, shape1=4*1000*0.01*0.1, shape2=4*1000*0.01*0.9)
#  numloci <- 8400
#  af <- replicate(numloci, getaf(0.001))
#  paste("{{", paste(sprintf("%f",af),collapse=","), "}}")

```

### Nemo Limitations to keep in mind:

* Certain arguments are temporal, but traits and the arguments we would want to manipulate are not.  
NOT POSSIBLE: selection_local_optima (@g0 {{0,0}} @g1000{{-1,1}})

* Also we can’t set the values of quantitative traits at different points in time (i.e. making them neutral for a while with quanti_allele_effect=0 and then having quanti_allele_effect=0.1).


### Create Standing variation files

```{r, eval=FALSE}
params$newfilenamebase <- NA
rootdir <- "nemoCoreSet2output/IM_2patch_base_stnvar_withfreq_v2/"
inidir <- "nemoCoreSet2ini/"
coresetparamname <- "nemoCoreSet2"
if(!dir.exists(inidir)){system(paste("mkdir", inidir))}

for (i in 1:nrow(params)){
  if(i%%10==0){print(c(i, "of", nrow(params)))}
  
  newfilenamebase <- paste("Coreset2popIM_stnvar_alpha=",sprintf("%f", params$alpha[i]), 
                       "_ntot=", sprintf("%03d",params$ntot[i]), 
                       "_Ne=",sprintf("%05d",params$Ne[i]),
                       "_mc=",round(params$mc[i],5), 
                       "_m=",round(params$m[i],5),
                       "_envi_var=",params$envi_var[i], 
                       "_mu=",params$mu[i], 
                       "_redun=", params$redundancy[i],
                       "_rep=", params$rep[i],
                       sep="")
  params$newfilenamebase[i] <- newfilenamebase
  newfilename <- paste(newfilenamebase,
                       c("_t0", "_t1"),
                       ".ini",
                       sep="")
  qtlmapname <- params$qtlmapfile[i]
  if(length(qtlmapname)>1){print("error qtlmapname >1 length"); break}
  newfnpath <- paste(inidir, newfilename,sep="")
  system(paste("cp IM_2patch_base_stnvar_t0.ini ",newfnpath[1]))
  system(paste("cp IM_2patch_base_stnvar_t1.ini ",newfnpath[2]))
  
 # WRITE t0
  write(paste("filename", newfilename[1]), newfnpath[1], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[1], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[1], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_environmental_variance", params$envi_var[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[1], append=TRUE)
              # this we set here, but the optima are at 0,0 at this timepoint
  write(paste("root_dir ",rootdir, newfilenamebase, sep=""), newfnpath[1], append=TRUE)   
  write(paste("ntrl_init_patch_freq",
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 8400
                                  )), 
        newfnpath[1], append=TRUE)
  write(paste("quanti_init_freq", 
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 840)), 
        newfnpath[1], append=TRUE)
 
 # WRITE t1 
  write(paste("source_pop ",rootdir,newfilenamebase,"/", newfilenamebase, "_t0.ini_1.bin", sep=""), newfnpath[2], append=TRUE) 
  write(paste("filename", newfilename[2]), newfnpath[2], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[2], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[2], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("root_dir ", rootdir, newfilenamebase, sep=""), newfnpath[2], append=TRUE)   
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[2], append=TRUE)

}
names(params)
head(params)

write.table(params,file = paste(coresetparamname, ".txt", sep=""), col.names=TRUE, row.names=FALSE)
save(params, file=paste(coresetparamname, ".Rdata", sep=""))
```

### TO DO
With Ne=1000 and mu=1e-06, only about 600 neutral loci were variable at the end of 50,000 generations. 

### Clustered vs. unclustered QTNs.
To address reviewer comments about clustering, propose to compare 2 simulations:

* 1 QTN in each QTL with mu = 1e-06
* 100 QTNs in each QTL with mu = 1e-08

Should give similar phenotypic evolution

### Estimating divergence from theory
(Email from Sam)
Hendry et al. 2001 gives an approximation that works pretty well when all loci are equal size and there is high recombination. But you need to know how much VG there is, and in practice this can't be predicted a-priori very easily, because it depends on whether the loci are swamping resistant or swamping prone. Probably there is a way to figure that out, but I haven't bothered yet (maybe I should?). it would involve:

1) are mutations swamping resistant?
2) derive their equilibrium frequency [0 or p]
3) convert to variance within populations (p * 1-p)
4) add mutation contributed by mutation-selection balance, which should be ~ Burger et al. 1989 stochastic version of Turelli 1984)

I think this would give a good estimation of VG...but haven't seen anyone do it this way formally. I'll see if I can find some time to try it out.

### Adding deleterious mutations

From Kim:

you should be able to calculate the average fitness loss by your deleterious mutations from the mean effect size and mutation rate. I'll have to look in my notes when I get to school. Something along the lines of a fitness loss of e^-U where U is the genomic mutation rate. And so I think that times your fecundity should give you something like your overall rate of replacement, and just aim to get that around 1 or greater.   For example, we had a genomic mutation rate (for deleterious loci) of 1  (1000 del loci * mut rate of 0.001)
So e^-1 was ~0.36 and that times a fecundity of 3 put expected fitness >1.
On top of that though, we increased fecundity even more so that populations could persist at even lower fitness levels. This latter part could kind of be anything, and we set our fecundity based on how much RAM the runs ended up needing.

```{r}
# if we have 1000 deleterious alleles
exp(-(mu*1000))
exp(-(mu*5000))

B = function(U,M){exp(-U/M)}

B(50000*0.8*1e-06, 50*log(1/(1-2*1e-10))*50000)
```

background selection yields access of rare alleles, negative Tajima's D

gamma distribution with different mean and variance for each exon

