---
title: "Core Sims"
author: "Katie Lotterhos"
date: "December 13, 2015"
output: html_document
---

In this document, I will make the dataframe for the core Nemo Simulations.

### Fitness function
We are using a Gaussian (quadratic) function to describe stabilizing selection on the phenotype ($z$) of individual ($i$) in population ($k$) with phenotypic optimum $\Theta$, and selection variance $\omega_k^2$:

$$ W_{z_{i,k}}= 1 - \frac{(z_{i,k}-\Theta_k)^2}{\omega_k^2}$$

As an R function:

```{r}
setwd("~/Dropbox/2015_12coresims")

w.zik <- function(zik, theta, omega.sq){
  1 - (zik - theta)^2/omega.sq
}
```

For the 2-patch model, we assume that each patch has an optimum of +1 and -1.  We assume that for a given number of loci that affect the trait(`ntot`) at the lowest level of redundancy, their effect sizes (`alpha`) are $1/(2*ntot)$.  For this, we can use the supplemental equation from Yeaman 2015 Am Nat to calculate the critical migration rate $m_c$.  At $m>m_c$, alleles are prone to swamping by migration.



```{r}
theta.1 <- 1
theta.2 <- -1
omega.sq <- 25
ntot <- c(2,10,20,60,100,400)
alpha <- 1/(2*ntot)


N=1000
m_crit <- function(alpha, theta.1, theta.2, omega.sq, N){
  w1bb <- 1#w.zik(4*alpha,theta.1, omega.sq)/
    #w.zik(4*alpha, theta.1, omega.sq)
  w1Bb <- w.zik(2*alpha,theta.1, omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  w1BB <- w.zik(0, theta.1,omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  
  w2bb <- w.zik(4*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2Bb <- w.zik(2*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2BB <- 1#w.zik(0, theta.2,omega.sq)/
    #w.zik(0, theta.2, omega.sq)
  
  w1bb;w1Bb; w1BB
  w2bb;w2Bb; w2BB
  return(1/(w1Bb/(w1Bb-w1BB*(1+(1/(4*N))))-w2Bb/(w2BB*(1+(1/(4*N)))-w2Bb))
  )
  }
mc <- m_crit(alpha, theta.1, theta.2, omega.sq, N)
cbind(ntot, alpha, mc=round(mc,5), lowmc=0.0100*round(mc,5), highmc=100*round(mc,5))
```

Note that for the smallest effect size, alleles will always be prone to swamping by migration.

### Levels 

Now, we want to extend this base set to cover redundancy (1.5 ntot, 2 ntot), mutation rate, Ne (1000, 5000), environmental variance (0,2,4), and 9 migration rates (0.01mc to 100mc) spanning the critical threshold, standing variation vs. new mutation, clustered vs. unclustered.   This gives a maximum of :
```{r}
length(ntot)* #levels polygenicity
  5* #levels redundancy 1, 2, 4, 6, 8
  #2* #levels Ne, 1000, 10000
  2* #levels envi var, 
  4* #levels migration, log scale, 1e-03, 1e-02, 1-e01 and mc
  4* #levels mutation/recombination, sample runif from Ne=0.0001, 0.001, 0.01, 0.1
  #2 # standing variation vs. new mutation
  3*  # selection strength omega.sq, sample runif from 5, 25, 50
  5 # replicates per level

14400 / 60 / 24 # 30 days on one cluster
```
levels, although note that some levels will not be possible (e.g. $m_c=0$ for ntot>400 and $m_c$>1.  We will do 3 replicates of each level.  

Eventually we will layer demography, clustered vs. unclustered QTNs, and deleterious mutations onto this framework.

### Genetic map ###

I've found that run time in Nemo increases with chromosome number (because crossing overs need to be computed for each one) and memory depends on chromosome length and resolution (because length determines the number of elements in the recombination lookup table (From `
void GeneticMap::setLookupTable`: "The lookup table for a 1M map at the 0.01 cM scale will have 10,000 elements")). In humans ($N_e$=10000), 1Mb (1,000,000 bases) correponds to 1 cM and a recombination rate $r=0.01$. In humans, the per-nucleotide mutation rate and recombination rate is assumed to be around $10^{-8}$ or $N_e\mu=N_er=10^{-4}$. 

Map size | r          | resolution  | number elements   | every X bases in humans
100 cM  |   10^{-4}  | 0.01 cM      | 10000             | 10000
10 cM   |   10^{-7}  | 0.00001 cM   | 1,000,000         | 10
10 cM   |   10^{-6}  | 0.0001 cM    | 100,000           | 100
1 cM    |   10^{-7} | 0.00001 cM  | 100,000             | 10
0.1 cM  |   10^{-8} | 0.000001 cM  | 100,000            | 1

`quanti_genetic_map_random` | `genetic_map_resolution` | Nemo indices
100 cM  | 0.000001   |    1- 10,000,000
100 cM  | 0.001     |    1 - 100000
10 cM   | 0.0001     |  1 - 100000

To make my own genetic map, use `quanti_genetic_map` and specify locations of markers on scale of 0 to 100,000.  The resolution will determine the recombination rate between them (I think).  

After scaling for Ne, we would want to pick 10^{-7} to capture humans.

We will simulate on 21 linkage groups, each 10cM long.  The 21st linkage groups will always have neutral loci.

I think in some cases we want to scale Ne, for example

Let's set the resolution of the map such that we capture recombination rates as low as 1e-06 (0.0001 cM).
`prefix_genetic_map_resolution 0.0001` (i.e., a distance of 1 then corresponds to a recombination rate of 1e-06 between two loci).

At this resolution, 100,000 indexes in Nemo corresponds to a map length of 10 cM (in humans, this corresponds to an index as 100 bases apart or total map length of 10 Mb).  This means that a the recombination among loci located at each end of the segment will be about $r=0.1$. 

With 8,400 total neutral loci, we have 400 neutral loci per chromosome. We COULD model a neutral locus about every 250 indexes, corresponding to an average recombination rate of 0.00025.  Instead, I propose to track groups of loci spread out along a chromosome, mimicking the data that we might acquire after sequence capture or RNAseq.  I propose to place SNPs over 40 â€œlocations" on each chromosome (each location approximately 100 indexes long and ~2500 indexes apart (r=0.0025 among locations)), getting 10 neutral SNPs per location.  

Each location will have 1 QTN in the center (relative index = 50) and 5 neutral SNPs evenly spaced on either side (indexes 1, X, X, 40, 49,QTN, 51, 60, X, X, 100), correpsonding to recombination among neighboring SNP-QTL of r=1e-06 and among QTL-SNP pairs at the end of the segment of r=5e-05

In humans, 100 indexes corresponds to 10,000 bases or 10Kb or the average length of a gene.


```{r}
  numseg <- 40#number of segments per chromosome
  seglength <- 100
  startseq <- 500 # starting location of first locus
  locs_start <- sort(round(seq(startseq, (100000-startseq-seglength), length.out=numseg)))
  locs_start
   all_locs <- lapply(locs_start, function(a) a:(a+seglength-1))
  locs_dist_between <- all_locs[[2]][1]-all_locs[[1]][seglength] # approx distance among locs
  locs_dist_between 
  
  length(unlist(all_locs)) #possible sites on a chrom
  #head(all_locs)
  #tail(all_locs, 1)
  bp <- unlist(all_locs)
  #head(all_locs)
  #tail(all_locs)

  ### Set up the genetic map for the first chromosome
  locs.df <- data.frame(linkage.group=1, bp=bp, contig=rep(1:numseg, each=seglength), type="no")
  locs.df$type<-as.character(locs.df$type)

  ### QTN equally spaced in center of each segment
  QTN_loc <- locs_start + seglength/2
  QTN_loc
  locs.df[which(bp %in% QTN_loc),]
  locs.df$type[bp %in% QTN_loc] <- "qtn"
  #locs.df[1:51,]

  ### neutral loci at increasing distance from QTN
  neut_dist_from_qtn <- c(1, 5, 10, 25, 49)
  neut1 <- sapply(QTN_loc, function(x)(x-neut_dist_from_qtn))
  neut2 <- sapply(QTN_loc, function(x)(x+neut_dist_from_qtn))
  locs.df$type[bp %in% c(neut1, neut2)] <- "neut"
  locs.df[1:101,]
  
  locs.df.chr1 <- locs.df
  for (i in 2:21){ #21 linkage groups
    l2 <- locs.df.chr1
    l2$linkage.group[1:nrow(l2)] <- i
    locs.df <- rbind(locs.df,l2)
      }
  
  head(locs.df, 101)
  tail(locs.df, 101)
```
#### The following code randomly assigns neutral loci and is not evaluated
```{r, eval=FALSE}
#   set.seed(4210)
#   newseed <- runif(21*20,min = 1000, max=99999)
#   k <- 0
#   locs.df$type <- "no"
#   for (chrom in 1:21){
#     for (seg in 1:20){
#       k <- k + 1
#       set.seed(newseed[k])
#       rows <- as.numeric(sample(rownames(locs.df)[locs.df$chr==chrom & locs.df$seqID==seg], size = 20, replace=FALSE))
#       locs.df$type[rows] = "neut"
#     }
#   }
# 
#   head(locs.df, 10)
#   tail(locs.df, 10)
#   head(locs.df[locs.df$type=="neut",], 50)
```

```{r}
  num.neut <- sum(locs.df$type=="neut") ## total number neutral loci
  num.neut
  num.qtn <- sum(locs.df$type=="qtn")
  num.qtn
```

```{r}
  #### Write neutral map to Nemo format ###
  coreMapNeutralFile <- "filesForNemoIni/coreMapNeutral.txt"
  for (i in 1:21){
    if(i==1){towrite <- paste("{{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="neut"], collapse=","),"}", sep="")
             write(towrite,coreMapNeutralFile, append=FALSE)}
    if(i==21){towrite <- paste("{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="neut"], collapse=","),"}}", sep="")
              write(towrite,coreMapNeutralFile, append=TRUE)}
    if(i>1 & i<21){
    towrite <- paste("{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="neut"], collapse=","),"}", sep="")
    write(towrite,coreMapNeutralFile, append=TRUE)
    }
    
  }#end loop


  #### Write qtl map to Nemo format ###
  coreMapQuantiFile <- "filesForNemoIni/coreMapQuanti.txt"
  #write("quanti_genetic_map \\\ ", coreMapQuantiFile)
  for (i in 1:21){
    if(i==1){towrite <- paste("{{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="qtn"], collapse=","),"}", sep="")
                 write(towrite,coreMapQuantiFile, append=FALSE)
             }
    if(i==21){towrite <- paste("{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="qtn"], collapse=","),"}}", sep="")
                  write(towrite,coreMapQuantiFile, append=TRUE)}
    if(i>1 & i<21){
    towrite <- paste("{",paste(locs.df$bp[locs.df$linkage.group==i & locs.df$type=="qtn"], collapse=","),"}", sep="")
        write(towrite,coreMapQuantiFile, append=TRUE)
    }
  }#end loop

  ### Write the locus dataframe to R format ###

  locs.df$pos <- (locs.df$linkage.group + (locs.df$bp)/100000)
  locs.df$col <- "grey"
  locs.df$col[locs.df$type=="qtn" & locs.df$linkage.group<21]="black"
  #write.table(locs.df, "geneticMaps/CoreSetGeneticMapFULL.txt", row.names=FALSE)
  write.table(locs.df[locs.df$type!="no",], "geneticMaps/CoreSetGeneticMapREDUCED.txt", row.names=FALSE)

  
```

```{r, fig.width=6}
par(mfrow=c(3,1), mar=c(5,1,1,1))
    plot(locs.df$pos, rep(1, length(locs.df$pos)), type="h", xlab= "locations of all loci (linkage groups)", yaxt="n", ylim=c(0.5,1), ylab="", bty="n", col=locs.df$col)

  cond <- which(locs.df$type=="neut" & locs.df$pos<2)
    plot(locs.df$bp[cond]/10000, rep(1, length(cond)), type="h", xlab= "locations of loci on\none linkage group (cM)", yaxt="n", ylim=c(0.5,1), ylab="", bty="n")

  cond <- which(locs.df$type!="no" & locs.df$pos<1.03)
    plot(locs.df$bp[cond]/10000, rep(1, length(cond)), type="h", xlab= "locations of neutral loci on one segment of one linkage group (cM)\nwith qtn in center", yaxt="n", ylim=c(0.5,1.2), ylab="", bty="n", col=as.numeric(factor(locs.df$type[cond])))
  arrows(0.055,1.2, 0.055,1, lwd=2)
```

Note that while the this regular spacing of neutral loci would not be realistic in a genomic dataset, it captures the various degrees of linkage that SNPs would have with a causal locus that would be present in a genomic dataset.  Thus we balance computational constraints (the number of loci we can realistically simulate) with the degrees of linkage that would be present in more dense data, and we can subsample from these SNPs to create datasets with less regular patterns.


### Initialization and number of generations

We can initialize qtns in Nemo to be "neutral" - i.e. have no effect on the trait.  This is acheived with:

* `quanti_init_model 0`
* `selection_local_optima {{0}{0}}`

I think we should burnin for 50000 generations with no selection until mutation-migration-drift balance (for all levels of Ne). THIS BURNIN IS CALLED `t0`.  Then, we save the population state and reload it in Nemo. We then change the local optima of selection to {{-1}{1}} and allow the population to adapt to the new optima for 50000 generations, output every 10000 generations. THIS TIME IS CALLED `t1.`

##### Selection from new mutation
* `quanti_allele_value 0` in t0
* `quanti_allele_value X or array` in t1
* `quanti_init_model 0` in t1

##### Standing genetic variation
* `quanti_allele_value X or array` in t0
* do not initilize trait quanti in t1

### Starting with a realistic allele frequency spectrum
Overview of options:

1) Initialize with observed spectrum from real data to parameterize beta distribution.
2) Simulate each replicate with a ridiculously large number of loci for a ridiculously long number of generations. (infeasible)
3) Simulate each value of $N_e\mu$ with a ridiculously large number of neutral loci for a ridiculously long number of generations in a single population. Use allele frequency distribution to parameterize beta distribution, use new random draw of beta distribution for each replicate.  If there are triallelic or haploid sites, can parameterize with multinomial distribution but I will need to check if Nemo can do this. For 1000 individuals and 3 alleles with frequency (0.5,0.4,0.1), this would be for example:
```{r}
#a <- 4Nem*p
#b <- 4Nem*(1-p)

rbeta(2, shape1=4*1000*0.01*0.1, shape2=4*1000*0.01*0.9)
#increase migration
rbeta(2, shape1=4*1000*0.1*0.1, shape2=4*1000*0.1*0.9)
#increase Ne
rbeta(2, shape1=4*10000*0.01*0.1, shape2=4*10000*0.01*0.9)
# decrease Nem
rbeta(2, shape1=4*1000*0.001*0.1, shape2=4*1000*0.001*0.9)
rbeta(2, shape1=4*1000*0.001*0.5, shape2=4*1000*0.001*0.5)
rbeta(2, shape1=4*1000*0.001*0.5, shape2=4*1000*0.001*0.5)
rbeta(2, shape1=4*1000*0.001*0.5, shape2=4*1000*0.001*0.5)
rbeta(2, shape1=4*1000*0.00001*0.5, shape2=4*1000*0.00001*0.5)
rbeta(2, shape1=4*1000*0.00001*0.9, shape2=4*1000*0.00001*0.1)
#the matrix must hold the patch-specific allele frequencies row-wise, and locus-specific frequencies column-wise. 
```
The multinomial approach is limited because Nemo cannot handle parameterizing allele frequencies for more than 2 alleles.

4) Simulate each value of $N_e\mu$ with a ridiculously large number of neutral loci for a ridiculously long number of generations in a single population with the number of individuals I need to fill the whole populations.  Use FSTAT individual data file to initiate Nemo.  I think this might be limited if I want to initialize QTL though. And intialization from a .bin store file wouldn't work, because I would want to exclude invariable loci.

I decided to go with 3.  This will avoid any issues with filtering allele frequencies from real data or ascertainment bias in real data. I need to do this. See note of triallelic data below.

*Initialize with observed spectrum from real data: From Vatsiou et al. 2015 MEC:*
For all scenarios, we used an initialization procedure
that samples allele frequencies from an island model at migrationâ€“mutationâ€“drift equilibrium. More precisely, all loci were initialized at the beginning of the simulations, t0 = 0, by sampling the allele frequencies of each locus from a beta distribution with parameters
$a = 4Nem*p$ and $b = 4Nem*(1-p)$, where p is the frequency in a migrant pool, which was derived from real human SNP data from noncoding regions, m is the migration rate, and Ne is the effective population size
(Wright)
Note from KEL: I believe the number of draws from the beta distribution equaled the number of populations in the data.

### Mutation and Ne levels
For mutation, let's assume we want to at least capture $N_e\mu$=1e-04, which would approximate humans.  This gives a per-base pair mutation rate $\mu_1$=1e-07 when $N_e_1=1000$ or $\mu_2$=1e-08 when $N_e_2=10000$, but based on the recombination resolution of our map (a haplotype block of 100 bp/index in humans), this corresponds to $\mu_1$/100=1e-05 and $\mu_2$/100=1e-06.  Assume our artifical increase in mutation is related to the map resolution. In other words, our mutation rate in Nemo is equal to the mutation rate of a haplotype block the size of the resolution of our genetic map.

Ne  | mu  | r | resolution  | Ne*mu
----|----|----|-----|-----       
1000 | $10^{-4}$ | $10^{-4}$ | 0.01 | 0.1
1000 |  $10^{-5}$ | $10^{-5}$ | 0.001 | 0.01
1000 |  $10^{-6}$ | $10^{-6}$ | 0.0001 | 0.001
1000 |  $10^{-7}$ | $10^{-7}$ | 0.00001 | 0.0001

Ne  | mu  | r | resolution  | Ne*mu
----|----|----|-----|-----       
10000 | $10^{-4}$ | $10^{-4}$ | 0.01 | 1
10000 |  $10^{-5}$ | $10^{-5}$ | 0.001 | 0.1
10000 |  $10^{-6}$ | $10^{-6}$ | 0.0001 | 0.01
10000 |  $10^{-7}$ | $10^{-7}$ | 0.00001 | 0.001

*From Thornton et al. 2013*
(Using a simulator similar to SLiM for a 100kB region)
We simulated a population of N=20,000 diploids with a neutral mutation rate of u=0.00125 per gamete per generation, and a recombination rate of r=0.00125 per diploid per generation. These values correspond to the scaled parameters $\theta=4N\mu=100$ and $\rho=4Nr=100$, and thus correspond to a â€˜â€˜typicalâ€™â€™ 100 kilobase region of the human genome. The mutation rate to causative (deleterious) mutations was $\mu_d=0.1\mu$ per gamete per generation. 

(Here, the logic is 0.00125/100,000 bases ~ $10^{-8}$)

**From a<**
The per-nucleotide mutation rate $u$ and recombination
rate $r$ were assumed to be equal to $10^{-7}$, implying values of $N_eu=N_er=10^{-4}$, which are appropriate for human populations (Li and Sadler 1991; Kong et al. 2002). Thus, because we used Ne = 1000 in the simulations and effective sizes for human populations are an order ofmagnitude larger (see, e.g., Charlesworth 2009), we increased the mutation and recombi- nation rates by an order ofmagnitude to simulate the genetic variation corresponding to a population that is 10 times larger. The scaled recombination rate is consistent with an average value of 1 cM/Mb in the genome.

A constant unstructured population of size N = Ne = 1000 individuals was run for 10,000 generations. This burn-in period ensured that allele frequencies were close to mutation-selection equilibrium. In the final burn-in generation, the population was expanded to 10,000 individuals to simulate a frequency distribution of genetic variants corresponding to an unscaled population size that was 10 times larger

### What to do about triallelic sites from trait initialization from new mutation
Note that initialization with `quanti_init_model 0` (from new mutation) can result in 3 possible states at a qtn: 0,-$\alpha$, or $\alpha$ (where $\alpha$ is effect size).  The question becomes how to deal with triallelic sites in genome scans that do not allow them?  Most methods only allow the number of copies of an allele.  We will do what we would do in real life - remove these loci from analysis.

### Simulating a distribution of QTL effect sizes for biallelic data (each locus has one effect size)
This approach assumes effect size for a SNP is +/-$\alpha$ (no matter size of alpha).

1) Draw qtn effect sizes from a distribution with arbitraily chosen parameters.  This has the limitation of the parameters being criticized.

2) For each value of $N_e\mu$ and total number of qtn, let effect sizes evolve under stabilizing selection in a single population(?) for a long time with mutational variance 0.02-0.04(?).  Sample from this distribution for qtn effect sizes for each replicate.  

2 seems like the best approach, but still have some question on how to implement.


### Simulating haplotype data and a distribution of quanti effect sizes at each locus
The SNP biallelic model may not be representative of human populations.  We would need to increase the resolution of the genetic map 100-fold to get a resolution representative of humans, and this is not computationally feasible if we want to simulate several linkage groups, unless we make the linkage groups 100-fold smaller (0.1cM or 10Mb).  I could try this, but would be too much linkage among qtl.

An alternative would be to simulate each site as a haplotype. This also has the advantage of allowing effect size to evolve for qtn haplotypes. The current version of Nemo has some different mutation models for quanti and ntrl traits. We would simulate quanti with the mutation variance model, which is essentially the infinite sites model.  Unlikely that a random draw from the continuous mutation distribution will result in the same effect size via two modes of mutation.  The options for neutral loci will then be:

1) Simulate as stepwise mutation model, in current implementation of Nemo. Not my preferred method, because will always produce fewer haplotypes for ntrl trait than the the quanti trait.

2) Include an indicator variable for whether or not quanti traits contribution to selection.  Then everything would be quanti but some would be neutral and have the same mutation model as the quanti traits.  Seems relatively simple.

Want to go with 2, but need to talk to Fred.  Seems like Jobran's already implemented this.


### Some core levels

```{r}
Ne <- c(1000, 1000, 1000,        10000, 10000, 10000)
mu <- c(1e-05, 1e-04, 1e-03,    1e-07, 1e-06, 1e-05) # levels??
nreps <- 5
cbind(Ne, Ne*mu)

(resolution <- mu*100)

envi_var <- c(2,4)
redundancy <- c(1, 2, 4, 6, 8)
sel.strength <- c(5,25,50)
#set.seed(198)
#seed <- sample(9999:99999, 3)
seed <- c(1,2,3)
expand.grid(envi_var=envi_var, 
                   resolution=resolution, 
            redundancy=redundancy, sel.strength)
```
For each of the above levels, we have multiple levels of allele effect sizes and multiple levels of migration rates.

### Make qtl input files for each nloc (3 reps)
For each of the three replicates within a number of qtns (5 levels), we will randomly create three genetic maps for the qtl

```{r}

nqtl_levels <- c()
nalpha_levels <- c()
for(i in seq_along(alpha)){
  nqtl_levels <- c(nqtl_levels, ntot[i]*redundancy)
  nalpha_levels <- c(nalpha_levels, rep(alpha[i],length(redundancy)))
}

qtlbase <- data.frame(nqtl_levels, nalpha_levels, redundancy=redundancy)

# genetic map can't handle more than 800 loci for now
if (sum(qtlbase$nqtl_levels>800)>0){
  qtlbase <- qtlbase[-(which(qtlbase$nqtl_levels>800)),]
}
  qtlbase
filename_qtlmap <- c() ; nloc_qtlmap <- c() ; alpha_qtlmap <- c(); redun <- c(); is.neut <- c()
for (i in seq_along(qtlbase$nqtl_levels)){
                    filename_qtlmap <- c(filename_qtlmap, paste("nqtl", sprintf("%03s",as.character(nqtl_levels[i])),
                    "redun",qtlbase$redundancy[i],
                     "alpha",sprintf("%f",qtlbase$nalpha_levels[i]),
                                              "rep",1:5,sep="_"))
  nloc_qtlmap <- c( nloc_qtlmap, rep(nqtl_levels[i],nreps))
  alpha_qtlmap <- c(alpha_qtlmap, rep(nalpha_levels[i],nreps))
  redun <- c(redun, rep(qtlbase$redundancy[i], nreps))
  }
filename_qtlmap
set.seed(4001)
qtlmapseed <- sample(9999:99999,length(alpha_qtlmap), replace=FALSE)
qtlmap <- data.frame(filename_qtlmap, nloc_qtlmap, qtlmapseed, alpha_qtlmap, 
                     rep=1:nreps,redun, outtext=NA)

for (i in seq_along(filename_qtlmap)){
  #print(filename_qtlmap[i])
   ### Assign qtns
  ### Want to sample chromosomes equally
  locs.df.reduced <- locs.df[locs.df$type!="no",]
  locs.df.reduced$is.neut <- TRUE
  locs.df.reduced$col="grey"
  locs.df.reduced$col[locs.df.reduced$linkage.group%%2==0] <- "grey60"
  head(locs.df[locs.df$type=="qtn",])
  
  nqtn <- nloc_qtlmap[i]
  chr.qtn <- sort(rep(1:20, length.out = nqtn)) #assign qtns to chromosome in order
  locs.df.reduced$effect <- 0
  #loop through chromosomes and randomly assign qtn position
      set.seed(qtlmapseed[i])
      newseed <- sample(999:99999, 800)
    for (j in seq_along(levels(factor(chr.qtn)))){
      #print(j)
      num <- sum(chr.qtn==j)
      possible.locs <- which(locs.df.reduced$type=="qtn" & locs.df.reduced$linkage.group==j)
      set.seed(newseed[j])
      locs <- sample(possible.locs,num)
      #print(locs)
      locs.df.reduced$effect[locs] <- alpha_qtlmap[i]
      locs.df.reduced$col[locs]<-"black"
      locs.df.reduced$is.neut[locs]<-FALSE
    } # end loop through j
  
    
  outtext <- paste("{{", 
               paste(locs.df.reduced$effect[locs.df.reduced$type=="qtn"], collapse=","),
               "}}")
  qtlmap$outtext[i] <- outtext
    write(outtext, paste("filesForNemoIni/",filename_qtlmap[i], sep=""))
    write.table(locs.df.reduced[order(locs.df.reduced$type),], file = paste("geneticMaps/",filename_qtlmap[i],".GeneticMap", sep=""), row.names=FALSE) 
} # this loop only takes a minute

names(qtlmap)
dim(qtlmap)
qtlmap[,1:6]
write.table(qtlmap,"geneticMaps/qtlmapdata.txt", row.names = FALSE)
```

### Expand grid to include levels of migration

```{r}
qtlmap$filename_qtlmap2 <- paste("filesForNemoIni/",filename_qtlmap, sep="") 
mig_levels <- c(1e-05, 1e-04, 1e-03, 1e-02, 1e-01)
omega.sq_levels <- c(5, 25, 50)
names(qtlmap)

i=0;params <- c()
for (a in 1:nrow(qtlmap)){  # each row is a replicate
  for (b in seq_along(Ne)){ # Ne and mu have the same length
    for (d in seq_along(omega.sq_levels)){
    mc<- m_crit(qtlmap$alpha_qtlmap[a], theta.1, theta.2, omega.sq_levels[d], Ne[b]) 
    if (mc<=0){
      mc1 <- NULL
    }else{mc1<-mc}
    mig2 <- c(mig_levels, mc1)
    (mig2 <- sort(mig2))
   
        out <- expand.grid(alpha=qtlmap$alpha_qtlmap[a], ntot=qtlmap$nloc_qtlmap[a], 
                           Ne=Ne[b], mu=mu[b],
                           mc=mc, 
                           m=mig2, 
                           redundancy = qtlmap$redun[a],
                           envi_var=envi_var,  rep=qtlmap$rep[a],
                           mapseed=qtlmap$qtlmapseed[a],
                           omega_sq=omega.sq_levels[d],
                           qtlmapfile = qtlmap$filename_qtlmap[a],
                           demog="IM2")
        out <- data.frame(out)
      if (i==1){
      params <- out
      }else{
        params <- rbind(params, out)
      }#end ifelse
      }#end if m
    }# end loop c
    }# end loop d
  }  # end loop b
}# end loop a 
# this only takes a minute or two

dim(params)
params$resolution <- params$mu*100 # set recombination equal to mutation
params$NeMu <- params$Ne*params$mu
params$gens_t1 <- "1, 1000, 10000, 50000"
params$gens_t0 <- 50000
params$numneut <- 8400
params$npops <- 2
params$typeselection <- "stnvar"
levels(factor(params$NeMu))
tail(head(params,180), 10)

### Get a prelim set

levels(factor(params$alpha))

### Preliminary set for large effect
set1 <- params$alpha==0.25 & params$Ne==1000 & params$redundancy==2 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set1)
params[set1,]

### Preliminary set for small effect
set2 <- params$alpha==0.005 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set2)
params[set2,]

set3 <- params$alpha==0.025 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set3)
params[set3,]

par(mfrow=c(3,1), mar=c(4,1,2,1))
  hist(log(params$m[set1],10), breaks=-5:1, main = "large effect mini set")
  arrows(log(params$mc[set1],10), 10, log(params$mc[set1],10), 0, lwd=3)
  
  hist(log(params$m[set3],10), breaks=-5:1, main = "medium effect mini set")
  arrows(log(params$mc[set3],10), 10, log(params$mc[set3],10), 0, lwd=3)

  hist(log(params$m[set2],10), breaks=-5:1, main = "small effect mini set")
  arrows(log(params$mc[set2],10), 10, log(params$mc[set2],10), 0, lwd=3)
```

### For now use a reduced set of parameters

```{r}
params2 <- params
params <- rbind(params[set1,], params[set3,], params[set2,])
#params<- params[params$m<1e-02 & params$m>2e-03,]
```

### Load in Allele Freq Spectrum table


```{r}
afbins <- read.table("createStnVar/TableAlleleFreqBins.txt", header=TRUE)
levels(factor(params$NeMu))

get_p <- function(NeMu){
  if(NeMu==0.0001)(p <- afbins$Neu.0.0001)
  if(NeMu==0.001)(p<- afbins$Neu.0.001)
  if(NeMu==0.01)(p<- afbins$Neu.0.01)
  if(NeMu>0.01)(p<- afbins$Neu.0.1)
  a <- sample(afbins$levs, size=1, prob=p)
  pout <- runif(1,min=a-0.049, max=a)
  if(pout<0.001){pout=0.001} # this is something to note
  return(sample(c(pout, 1-pout),1))
}

get_beta <- function(Ne, m, u, npops){
  p <- get_p(round(Ne*u,4))
  a <- 4*Ne*m*p
  b <- 4*Ne*m*(1-p)
  return(rbeta(npops, shape1=a, shape2=b))
}

nemo_ntrl_init_freq <- function(Ne, m, u, npops, nloci){
  bob <- c()
  if(npops>1){
      freqmat <- replicate(n=(nloci),
                           get_beta(Ne,m,u,npops))
    for (i in 1:npops){
      bob[i] <- paste("{",paste(sprintf("%f",
                freqmat[i,]), collapse=", "),"}")
    }
  }
  if(npops==1){
    freqmat <- replicate(n=nloci, get_p(round(Ne*u,4)))
    bob <- paste("{",paste(sprintf("%f", freqmat), collapse=", "),"}")
  }
  return(paste("{", paste(bob, collapse="") ,"}"))
}

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-07,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.0001,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

nemo_ntrl_init_freq(1000,m=1e-04,1e-05,2, 10)
nemo_ntrl_init_freq(1000,m=1e-04,1e-05,1, 10)
# #
# b <- 4Nem*(1-p)
# rbeta(2, shape1=4*1000*0.01*0.1, shape2=4*1000*0.01*0.9)
#  numloci <- 8400
#  af <- replicate(numloci, getaf(0.001))
#  paste("{{", paste(sprintf("%f",af),collapse=","), "}}")

```

### Nemo Limitations to keep in mind:

* Certain arguments are temporal, but traits and the arguments we would want to manipulate are not.  
NOT POSSIBLE: selection_local_optima (@g0 {{0,0}} @g1000{{-1,1}})

* Also we canâ€™t set the values of quantitative traits at different points in time (i.e. making them neutral for a while with quanti_allele_effect=0 and then having quanti_allele_effect=0.1).


### Create Standing variation files

```{r, eval=FALSE}
params$newfilenamebase <- NA
rootdir <- "nemoCoreSet2output/IM_2patch_base_stnvar_withfreq_v2/"
inidir <- "nemoCoreSet2ini/"
coresetparamname <- "nemoCoreSet2"
if(!dir.exists(inidir)){system(paste("mkdir", inidir))}

for (i in 1:nrow(params)){
  if(i%%10==0){print(c(i, "of", nrow(params)))}
  
  newfilenamebase <- paste("Coreset2popIM_stnvar_alpha=",sprintf("%f", params$alpha[i]), 
                       "_ntot=", sprintf("%03d",params$ntot[i]), 
                       "_Ne=",sprintf("%05d",params$Ne[i]),
                       "_mc=",round(params$mc[i],5), 
                       "_m=",round(params$m[i],5),
                       "_envi_var=",params$envi_var[i], 
                       "_mu=",params$mu[i], 
                       "_redun=", params$redundancy[i],
                       "_rep=", params$rep[i],
                       sep="")
  params$newfilenamebase[i] <- newfilenamebase
  newfilename <- paste(newfilenamebase,
                       c("_t0", "_t1"),
                       ".ini",
                       sep="")
  qtlmapname <- params$qtlmapfile[i]
  if(length(qtlmapname)>1){print("error qtlmapname >1 length"); break}
  newfnpath <- paste(inidir, newfilename,sep="")
  system(paste("cp IM_2patch_base_stnvar_t0.ini ",newfnpath[1]))
  system(paste("cp IM_2patch_base_stnvar_t1.ini ",newfnpath[2]))
  
 # WRITE t0
  write(paste("filename", newfilename[1]), newfnpath[1], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[1], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[1], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_environmental_variance", params$envi_var[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[1], append=TRUE)
              # this we set here, but the optima are at 0,0 at this timepoint
  write(paste("root_dir ",rootdir, newfilenamebase, sep=""), newfnpath[1], append=TRUE)   
  write(paste("ntrl_init_patch_freq",
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 8400
                                  )), 
        newfnpath[1], append=TRUE)
  write(paste("quanti_init_freq", 
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 840)), 
        newfnpath[1], append=TRUE)
 
 # WRITE t1 
  write(paste("source_pop ",rootdir,newfilenamebase,"/", newfilenamebase, "_t0.ini_1.bin", sep=""), newfnpath[2], append=TRUE) 
  write(paste("filename", newfilename[2]), newfnpath[2], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[2], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[2], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("root_dir ", rootdir, newfilenamebase, sep=""), newfnpath[2], append=TRUE)   
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[2], append=TRUE)

}
names(params)
head(params)

write.table(params,file = paste(coresetparamname, ".txt", sep=""), col.names=TRUE, row.names=FALSE)
save(params, file=paste(coresetparamname, ".Rdata", sep=""))
```

### TO DO
With Ne=1000 and mu=1e-06, only about 600 neutral loci were variable at the end of 50,000 generations. 

### Clustered vs. unclustered QTNs.
To address reviewer comments about clustering, propose to compare 2 simulations:

* 1 QTN in each QTL with mu = 1e-06
* 100 QTNs in each QTL with mu = 1e-08

Should give similar phenotypic evolution

### Estimating divergence from theory
(Email from Sam)
Hendry et al. 2001 gives an approximation that works pretty well when all loci are equal size and there is high recombination. But you need to know how much VG there is, and in practice this can't be predicted a-priori very easily, because it depends on whether the loci are swamping resistant or swamping prone. Probably there is a way to figure that out, but I haven't bothered yet (maybe I should?). it would involve:

1) are mutations swamping resistant?
2) derive their equilibrium frequency [0 or p]
3) convert to variance within populations (p * 1-p)
4) add mutation contributed by mutation-selection balance, which should be ~ Burger et al. 1989 stochastic version of Turelli 1984)

I think this would give a good estimation of VG...but haven't seen anyone do it this way formally. I'll see if I can find some time to try it out.

### Adding deleterious mutations

From Kim:

you should be able to calculate the average fitness loss by your deleterious mutations from the mean effect size and mutation rate. I'll have to look in my notes when I get to school. Something along the lines of a fitness loss of e^-U where U is the genomic mutation rate. And so I think that times your fecundity should give you something like your overall rate of replacement, and just aim to get that around 1 or greater.   For example, we had a genomic mutation rate (for deleterious loci) of 1  (1000 del loci * mut rate of 0.001)
So e^-1 was ~0.36 and that times a fecundity of 3 put expected fitness >1.
On top of that though, we increased fecundity even more so that populations could persist at even lower fitness levels. This latter part could kind of be anything, and we set our fecundity based on how much RAM the runs ended up needing.

```{r}
# if we have 1000 deleterious alleles
exp(-(mu*1000))
exp(-(mu*5000))

B = function(U,M){exp(-U/M)}

B(50000*0.8*1e-06, 50*log(1/(1-2*1e-10))*50000)
```

background selection yields access of rare alleles, negative Tajima's D

gamma distribution with different mean and variance for each exon

