---
title: "Core Sims"
author: "Katie Lotterhos"
date: "December 13, 2015"
output: html_document
---

In this document, I will make the dataframe for the core Nemo Simulations.

### Fitness function
We are using a Gaussian (quadratic) function to describe stabilizing selection on the phenotype ($z$) of individual ($i$) in population ($k$) with phenotypic optimum $\Theta$, and selection variance $\omega_k^2$:

$$ W_{z_{i,k}}= 1 - \frac{(z_{i,k}-\Theta_k)^2}{\omega_k^2}$$

As an R function:

```{r}
setwd("~/Dropbox/2015_12coresims")

w.zik <- function(zik, theta, omega.sq){
  1 - (zik - theta)^2/omega.sq
}
```

For the 2-patch model, we assume that each patch has an optimum of +1 and -1.  We assume that for a given number of loci that affect the trait(`ntot`) at the lowest level of redundancy, their effect sizes (`alpha`) are $1/(2*ntot)$.  For this, we can use the supplemental equation from Yeaman 2015 Am Nat to calculate the critical migration rate $m_c$.  At $m>m_c$, alleles are prone to swamping by migration.



```{r}
theta.1 <- 1
theta.2 <- -1
omega.sq <- 25
ntot <- c(1, 2, 10,20,50,100,500)
alpha <- 1/(2*ntot)
# alpha is effect size input to SliM


N=1000
m_crit <- function(alpha, theta.1, theta.2, omega.sq, N){
  w1bb <- 1#w.zik(4*alpha,theta.1, omega.sq)/
    #w.zik(4*alpha, theta.1, omega.sq)
  w1Bb <- w.zik(2*alpha,theta.1, omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  w1BB <- w.zik(0, theta.1,omega.sq)/
    w.zik(4*alpha, theta.1, omega.sq)
  
  w2bb <- w.zik(4*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2Bb <- w.zik(2*alpha,theta.2, omega.sq)/
    w.zik(0, theta.2, omega.sq)
  w2BB <- 1#w.zik(0, theta.2,omega.sq)/
    #w.zik(0, theta.2, omega.sq)
  
  w1bb;w1Bb; w1BB
  w2bb;w2Bb; w2BB
  return(1/(w1Bb/(w1Bb-w1BB*(1+(1/(4*N))))-w2Bb/(w2BB*(1+(1/(4*N)))-w2Bb))
  )
  }
mc <- m_crit(alpha, theta.1, theta.2, omega.sq, N)
cbind(ntot, alpha, mc=round(mc,5), lowmc=0.0100*round(mc,5), highmc=100*round(mc,5))
```

Note that for the smallest effect size, alleles will always be prone to swamping by migration.

### Levels 

Now, we want to extend this base set to cover redundancy (1.5 ntot, 2 ntot), mutation rate, Ne (1000, 5000), environmental variance (0,2,4), and 9 migration rates (0.01mc to 100mc) spanning the critical threshold, standing variation vs. new mutation, clustered vs. unclustered.   This gives a maximum of :
```{r}
length(ntot)* #levels polygenicity
  3* #levels redundancy 1, 2, 4,
  2* #levels Ne, 1000, 10000
  2* #levels envi var, 
  4* #levels migration, log scale, 1e-03, 1e-02, 1-e01 and mc
  4* #levels mutation/recombination, sample runif from Ne=0.0001, 0.001, 0.01, 0.1
  #2 # standing variation vs. new mutation
  3*  # selection strength omega.sq, sample runif from 5, 25, 50
  5 # replicates per level

20160*3 / 60  # minutes per server
1000/60 # hours needed
```
levels, although note that some levels will not be possible (e.g. $m_c=0$ for ntot>400 and $m_c$>1.  We will do 3 replicates of each level.  

Eventually we will layer demography, clustered vs. unclustered QTNs, and deleterious mutations onto this framework.


### Mutation and Ne levels
For mutation, let's assume we want to at least capture $N_e\mu$=1e-04, which would approximate humans.  This gives a per-base pair mutation rate $\mu_1$=1e-07 when $N_e_1=1000$ or $\mu_2$=1e-08 when $N_e_2=10000$, but based on the recombination resolution of our map (a haplotype block of 100 bp/index in humans), this corresponds to $\mu_1$/100=1e-05 and $\mu_2$/100=1e-06.  Assume our artifical increase in mutation is related to the map resolution. In other words, our mutation rate in Nemo is equal to the mutation rate of a haplotype block the size of the resolution of our genetic map.

Ne  | mu  | r | resolution  | Ne*mu
----|----|----|-----|-----       
1000 | $10^{-4}$ | $10^{-4}$ | 0.01 | 0.1
1000 |  $10^{-5}$ | $10^{-5}$ | 0.001 | 0.01
1000 |  $10^{-6}$ | $10^{-6}$ | 0.0001 | 0.001
1000 |  $10^{-7}$ | $10^{-7}$ | 0.00001 | 0.0001

Ne  | mu  | r | resolution  | Ne*mu
----|----|----|-----|-----       
10000 | $10^{-4}$ | $10^{-4}$ | 0.01 | 1
10000 |  $10^{-5}$ | $10^{-5}$ | 0.001 | 0.1
10000 |  $10^{-6}$ | $10^{-6}$ | 0.0001 | 0.01
10000 |  $10^{-7}$ | $10^{-7}$ | 0.00001 | 0.001

*From Thornton et al. 2013*
(Using a simulator similar to SLiM for a 100kB region)
We simulated a population of N=20,000 diploids with a neutral mutation rate of u=0.00125 per gamete per generation, and a recombination rate of r=0.00125 per diploid per generation. These values correspond to the scaled parameters $\theta=4N\mu=100$ and $\rho=4Nr=100$, and thus correspond to a ‘‘typical’’ 100 kilobase region of the human genome. The mutation rate to causative (deleterious) mutations was $\mu_d=0.1\mu$ per gamete per generation. 

(Here, the logic is 0.00125/100,000 bases ~ $10^{-8}$)

**From a<**
The per-nucleotide mutation rate $u$ and recombination
rate $r$ were assumed to be equal to $10^{-7}$, implying values of $N_eu=N_er=10^{-4}$, which are appropriate for human populations (Li and Sadler 1991; Kong et al. 2002). Thus, because we used Ne = 1000 in the simulations and effective sizes for human populations are an order ofmagnitude larger (see, e.g., Charlesworth 2009), we increased the mutation and recombi- nation rates by an order ofmagnitude to simulate the genetic variation corresponding to a population that is 10 times larger. The scaled recombination rate is consistent with an average value of 1 cM/Mb in the genome.

A constant unstructured population of size N = Ne = 1000 individuals was run for 10,000 generations. This burn-in period ensured that allele frequencies were close to mutation-selection equilibrium. In the final burn-in generation, the population was expanded to 10,000 individuals to simulate a frequency distribution of genetic variants corresponding to an unscaled population size that was 10 times larger

### What to do about triallelic sites from trait initialization from new mutation
Note that initialization with `quanti_init_model 0` (from new mutation) can result in 3 possible states at a qtn: 0,-$\alpha$, or $\alpha$ (where $\alpha$ is effect size).  The question becomes how to deal with triallelic sites in genome scans that do not allow them?  Most methods only allow the number of copies of an allele.  We will do what we would do in real life - remove these loci from analysis.

### Simulating a distribution of QTL effect sizes for biallelic data (each locus has one effect size)
This approach assumes effect size for a SNP is +/-$\alpha$ (no matter size of alpha).

1) Draw qtn effect sizes from a distribution with arbitraily chosen parameters.  This has the limitation of the parameters being criticized.

2) For each value of $N_e\mu$ and total number of qtn, let effect sizes evolve under stabilizing selection in a single population(?) for a long time with mutational variance 0.02-0.04(?).  Sample from this distribution for qtn effect sizes for each replicate.  

2 seems like the best approach, but still have some question on how to implement.


### Simulating haplotype data and a distribution of quanti effect sizes at each locus
The SNP biallelic model may not be representative of human populations.  We would need to increase the resolution of the genetic map 100-fold to get a resolution representative of humans, and this is not computationally feasible if we want to simulate several linkage groups, unless we make the linkage groups 100-fold smaller (0.1cM or 10Mb).  I could try this, but would be too much linkage among qtl.

An alternative would be to simulate each site as a haplotype. This also has the advantage of allowing effect size to evolve for qtn haplotypes. The current version of Nemo has some different mutation models for quanti and ntrl traits. We would simulate quanti with the mutation variance model, which is essentially the infinite sites model.  Unlikely that a random draw from the continuous mutation distribution will result in the same effect size via two modes of mutation.  The options for neutral loci will then be:

1) Simulate as stepwise mutation model, in current implementation of Nemo. Not my preferred method, because will always produce fewer haplotypes for ntrl trait than the the quanti trait.

2) Include an indicator variable for whether or not quanti traits contribution to selection.  Then everything would be quanti but some would be neutral and have the same mutation model as the quanti traits.  Seems relatively simple.

Want to go with 2, but need to talk to Fred.  Seems like Jobran's already implemented this.


### Some core levels

```{r}
Ne <- c(1000, 1000, 1000, 1000,         10000, 10000)
mu <- c(1e-06, 1e-05, 1e-04, 1e-03,     1e-06, 1e-05) # levels??
nreps <- 5
cbind(Ne, Ne*mu)

(resolution <- 0.001)

envi_var <- c(0, 2,4)
redundancy <- c(1, 2, 4)
sel.strength <- c(5,25,50)
#set.seed(198)
#seed <- sample(9999:99999, 3)
seed <- c(1,2,3)
expand.grid(envi_var=envi_var, 
                   resolution=resolution, 
            redundancy=redundancy, sel.strength=sel.strength)
```
For each of the above levels, we have multiple levels of allele effect sizes and multiple levels of migration rates.

### Make qtl input files for each nloc (3 reps)
For each of the three replicates within a number of qtns (5 levels), we will randomly create three genetic maps for the qtl

```{r}

nqtl_levels <- c()
nalpha_levels <- c()
for(i in seq_along(alpha)){
  nqtl_levels <- c(nqtl_levels, ntot[i]*redundancy)
  nalpha_levels <- c(nalpha_levels, rep(alpha[i],length(redundancy)))
}

qtlbase <- data.frame(nqtl_levels, nalpha_levels, redundancy=redundancy)

# genetic map can't handle more than 800 loci for now
if (sum(qtlbase$nqtl_levels>800)>0){
  qtlbase <- qtlbase[-(which(qtlbase$nqtl_levels>800)),]
}
  qtlbase
filename_qtlmap <- c() ; nloc_qtlmap <- c() ; alpha_qtlmap <- c(); redun <- c(); is.neut <- c()
for (i in seq_along(qtlbase$nqtl_levels)){
                    filename_qtlmap <- c(filename_qtlmap, paste("nqtl", sprintf("%03s",as.character(nqtl_levels[i])),
                    "redun",qtlbase$redundancy[i],
                     "alpha",sprintf("%f",qtlbase$nalpha_levels[i]),
                                              "rep",1:5,sep="_"))
  nloc_qtlmap <- c( nloc_qtlmap, rep(nqtl_levels[i],nreps))
  alpha_qtlmap <- c(alpha_qtlmap, rep(nalpha_levels[i],nreps))
  redun <- c(redun, rep(qtlbase$redundancy[i], nreps))
  }
filename_qtlmap
set.seed(4001)
qtlmapseed <- sample(9999:99999,length(alpha_qtlmap), replace=FALSE)
qtlmap <- data.frame(filename_qtlmap, nloc_qtlmap, qtlmapseed, alpha_qtlmap, 
                     rep=1:nreps,redun, outtext=NA)

for (i in seq_along(filename_qtlmap)){
  #print(filename_qtlmap[i])
   ### Assign qtns
  ### Want to sample chromosomes equally
  locs.df.reduced <- locs.df[locs.df$type!="no",]
  locs.df.reduced$is.neut <- TRUE
  locs.df.reduced$col="grey"
  locs.df.reduced$col[locs.df.reduced$linkage.group%%2==0] <- "grey60"
  head(locs.df[locs.df$type=="qtn",])
  
  nqtn <- nloc_qtlmap[i]
  chr.qtn <- sort(rep(1:20, length.out = nqtn)) #assign qtns to chromosome in order
  locs.df.reduced$effect <- 0
  #loop through chromosomes and randomly assign qtn position
      set.seed(qtlmapseed[i])
      newseed <- sample(999:99999, 800)
    for (j in seq_along(levels(factor(chr.qtn)))){
      #print(j)
      num <- sum(chr.qtn==j)
      possible.locs <- which(locs.df.reduced$type=="qtn" & locs.df.reduced$linkage.group==j)
      set.seed(newseed[j])
      locs <- sample(possible.locs,num)
      #print(locs)
      locs.df.reduced$effect[locs] <- alpha_qtlmap[i]
      locs.df.reduced$col[locs]<-"black"
      locs.df.reduced$is.neut[locs]<-FALSE
    } # end loop through j
  
    
  outtext <- paste("{{", 
               paste(locs.df.reduced$effect[locs.df.reduced$type=="qtn"], collapse=","),
               "}}")
  qtlmap$outtext[i] <- outtext
    write(outtext, paste("filesForNemoIni/",filename_qtlmap[i], sep=""))
    write.table(locs.df.reduced[order(locs.df.reduced$type),], file = paste("geneticMaps/",filename_qtlmap[i],".GeneticMap", sep=""), row.names=FALSE) 
} # this loop only takes a minute

names(qtlmap)
dim(qtlmap)
qtlmap[,1:6]
write.table(qtlmap,"geneticMaps/qtlmapdata.txt", row.names = FALSE)
```

### Expand grid to include levels of migration

```{r}
qtlmap$filename_qtlmap2 <- paste("filesForNemoIni/",filename_qtlmap, sep="") 
mig_levels <- c(1e-05, 1e-04, 1e-03, 1e-02, 1e-01)
omega.sq_levels <- c(5, 25, 50)
names(qtlmap)

i=0;params <- c()
for (a in 1:nrow(qtlmap)){  # each row is a replicate
  for (b in seq_along(Ne)){ # Ne and mu have the same length
    for (d in seq_along(omega.sq_levels)){
    mc<- m_crit(qtlmap$alpha_qtlmap[a], theta.1, theta.2, omega.sq_levels[d], Ne[b]) 
    if (mc<=0){
      mc1 <- NULL
    }else{mc1<-mc}
    mig2 <- c(mig_levels, mc1)
    (mig2 <- sort(mig2))
   
        out <- expand.grid(alpha=qtlmap$alpha_qtlmap[a], ntot=qtlmap$nloc_qtlmap[a], 
                           Ne=Ne[b], mu=mu[b],
                           mc=mc, 
                           m=mig2, 
                           redundancy = qtlmap$redun[a],
                           envi_var=envi_var,  rep=qtlmap$rep[a],
                           mapseed=qtlmap$qtlmapseed[a],
                           omega_sq=omega.sq_levels[d],
                           qtlmapfile = qtlmap$filename_qtlmap[a],
                           demog="IM2")
        out <- data.frame(out)
      if (i==1){
      params <- out
      }else{
        params <- rbind(params, out)
      }#end ifelse
      }#end if m
    }# end loop c
    }# end loop d
  }  # end loop b
}# end loop a 
# this only takes a minute or two

dim(params)
params$resolution <- params$mu*100 # set recombination equal to mutation
params$NeMu <- params$Ne*params$mu
params$gens_t1 <- "1, 1000, 10000, 50000"
params$gens_t0 <- 50000
params$numneut <- 8400
params$npops <- 2
params$typeselection <- "stnvar"
levels(factor(params$NeMu))
tail(head(params,180), 10)

### Get a prelim set

levels(factor(params$alpha))

### Preliminary set for large effect
set1 <- params$alpha==0.25 & params$Ne==1000 & params$redundancy==2 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set1)
params[set1,]

### Preliminary set for small effect
set2 <- params$alpha==0.005 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set2)
params[set2,]

set3 <- params$alpha==0.025 & params$Ne==1000 & params$redundancy==8 & params$envi_var==2 & (params$mu==1e-05) & params$omega_sq==25
sum(set3)
params[set3,]

par(mfrow=c(3,1), mar=c(4,1,2,1))
  hist(log(params$m[set1],10), breaks=-5:1, main = "large effect mini set")
  arrows(log(params$mc[set1],10), 10, log(params$mc[set1],10), 0, lwd=3)
  
  hist(log(params$m[set3],10), breaks=-5:1, main = "medium effect mini set")
  arrows(log(params$mc[set3],10), 10, log(params$mc[set3],10), 0, lwd=3)

  hist(log(params$m[set2],10), breaks=-5:1, main = "small effect mini set")
  arrows(log(params$mc[set2],10), 10, log(params$mc[set2],10), 0, lwd=3)
```

### For now use a reduced set of parameters

```{r}
params2 <- params
params <- rbind(params[set1,], params[set3,], params[set2,])
#params<- params[params$m<1e-02 & params$m>2e-03,]
```

### Load in Allele Freq Spectrum table


```{r}
afbins <- read.table("createStnVar/TableAlleleFreqBins.txt", header=TRUE)
levels(factor(params$NeMu))

get_p <- function(NeMu){
  if(NeMu==0.0001)(p <- afbins$Neu.0.0001)
  if(NeMu==0.001)(p<- afbins$Neu.0.001)
  if(NeMu==0.01)(p<- afbins$Neu.0.01)
  if(NeMu>0.01)(p<- afbins$Neu.0.1)
  a <- sample(afbins$levs, size=1, prob=p)
  pout <- runif(1,min=a-0.049, max=a)
  if(pout<0.001){pout=0.001} # this is something to note
  return(sample(c(pout, 1-pout),1))
}

get_beta <- function(Ne, m, u, npops){
  p <- get_p(round(Ne*u,4))
  a <- 4*Ne*m*p
  b <- 4*Ne*m*(1-p)
  return(rbeta(npops, shape1=a, shape2=b))
}

nemo_ntrl_init_freq <- function(Ne, m, u, npops, nloci){
  bob <- c()
  if(npops>1){
      freqmat <- replicate(n=(nloci),
                           get_beta(Ne,m,u,npops))
    for (i in 1:npops){
      bob[i] <- paste("{",paste(sprintf("%f",
                freqmat[i,]), collapse=", "),"}")
    }
  }
  if(npops==1){
    freqmat <- replicate(n=nloci, get_p(round(Ne*u,4)))
    bob <- paste("{",paste(sprintf("%f", freqmat), collapse=", "),"}")
  }
  return(paste("{", paste(bob, collapse="") ,"}"))
}

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-07,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.01,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

freq_start <- replicate(n=(8400+40*21), get_beta(1000,0.0001,1e-04,2))
hist(freq_start[1,], breaks=seq(0,1,0.05))
hist(freq_start[1,]- freq_start[2,])

nemo_ntrl_init_freq(1000,m=1e-04,1e-05,2, 10)
nemo_ntrl_init_freq(1000,m=1e-04,1e-05,1, 10)
# #
# b <- 4Nem*(1-p)
# rbeta(2, shape1=4*1000*0.01*0.1, shape2=4*1000*0.01*0.9)
#  numloci <- 8400
#  af <- replicate(numloci, getaf(0.001))
#  paste("{{", paste(sprintf("%f",af),collapse=","), "}}")

```

### Nemo Limitations to keep in mind:

* Certain arguments are temporal, but traits and the arguments we would want to manipulate are not.  
NOT POSSIBLE: selection_local_optima (@g0 {{0,0}} @g1000{{-1,1}})

* Also we can’t set the values of quantitative traits at different points in time (i.e. making them neutral for a while with quanti_allele_effect=0 and then having quanti_allele_effect=0.1).


### Create Standing variation files

```{r, eval=FALSE}
params$newfilenamebase <- NA
rootdir <- "nemoCoreSet2output/IM_2patch_base_stnvar_withfreq_v2/"
inidir <- "nemoCoreSet2ini/"
coresetparamname <- "nemoCoreSet2"
if(!dir.exists(inidir)){system(paste("mkdir", inidir))}

for (i in 1:nrow(params)){
  if(i%%10==0){print(c(i, "of", nrow(params)))}
  
  newfilenamebase <- paste("Coreset2popIM_stnvar_alpha=",sprintf("%f", params$alpha[i]), 
                       "_ntot=", sprintf("%03d",params$ntot[i]), 
                       "_Ne=",sprintf("%05d",params$Ne[i]),
                       "_mc=",round(params$mc[i],5), 
                       "_m=",round(params$m[i],5),
                       "_envi_var=",params$envi_var[i], 
                       "_mu=",params$mu[i], 
                       "_redun=", params$redundancy[i],
                       "_rep=", params$rep[i],
                       sep="")
  params$newfilenamebase[i] <- newfilenamebase
  newfilename <- paste(newfilenamebase,
                       c("_t0", "_t1"),
                       ".ini",
                       sep="")
  qtlmapname <- params$qtlmapfile[i]
  if(length(qtlmapname)>1){print("error qtlmapname >1 length"); break}
  newfnpath <- paste(inidir, newfilename,sep="")
  system(paste("cp IM_2patch_base_stnvar_t0.ini ",newfnpath[1]))
  system(paste("cp IM_2patch_base_stnvar_t1.ini ",newfnpath[2]))
  
 # WRITE t0
  write(paste("filename", newfilename[1]), newfnpath[1], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[1], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[1], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_environmental_variance", params$envi_var[i]), newfnpath[1], append=TRUE)
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[1], append=TRUE)
              # this we set here, but the optima are at 0,0 at this timepoint
  write(paste("root_dir ",rootdir, newfilenamebase, sep=""), newfnpath[1], append=TRUE)   
  write(paste("ntrl_init_patch_freq",
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 8400
                                  )), 
        newfnpath[1], append=TRUE)
  write(paste("quanti_init_freq", 
              nemo_ntrl_init_freq(Ne = params$Ne[i],
                                  m = params$m[i],
                                  u = params$mu[i],
                                  npops = 1,
                                  nloci = 840)), 
        newfnpath[1], append=TRUE)
 
 # WRITE t1 
  write(paste("source_pop ",rootdir,newfilenamebase,"/", newfilenamebase, "_t0.ini_1.bin", sep=""), newfnpath[2], append=TRUE) 
  write(paste("filename", newfilename[2]), newfnpath[2], append=TRUE)
  write(paste("patch_capacity", params$Ne[i]), newfnpath[2], append=TRUE)
  write(paste("breed_disperse_rate", sprintf("%f",params$m[i])), newfnpath[2], append=TRUE)
  #write(paste("quanti_loci 840"), newfnpath, append=TRUE)
  write(paste("quanti_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("ntrl_mutation_rate", params$mu[i]), newfnpath[2], append=TRUE)
  write(paste("root_dir ", rootdir, newfilenamebase, sep=""), newfnpath[2], append=TRUE)   
  write(paste("quanti_allele_value @filesForNemoIni/" , qtlmapname,sep=""), newfnpath[2], append=TRUE)

}
names(params)
head(params)

write.table(params,file = paste(coresetparamname, ".txt", sep=""), col.names=TRUE, row.names=FALSE)
save(params, file=paste(coresetparamname, ".Rdata", sep=""))
```

### TO DO
With Ne=1000 and mu=1e-06, only about 600 neutral loci were variable at the end of 50,000 generations. 

### Clustered vs. unclustered QTNs.
To address reviewer comments about clustering, propose to compare 2 simulations:

* 1 QTN in each QTL with mu = 1e-06
* 100 QTNs in each QTL with mu = 1e-08

Should give similar phenotypic evolution

### Estimating divergence from theory
(Email from Sam)
Hendry et al. 2001 gives an approximation that works pretty well when all loci are equal size and there is high recombination. But you need to know how much VG there is, and in practice this can't be predicted a-priori very easily, because it depends on whether the loci are swamping resistant or swamping prone. Probably there is a way to figure that out, but I haven't bothered yet (maybe I should?). it would involve:

1) are mutations swamping resistant?
2) derive their equilibrium frequency [0 or p]
3) convert to variance within populations (p * 1-p)
4) add mutation contributed by mutation-selection balance, which should be ~ Burger et al. 1989 stochastic version of Turelli 1984)

I think this would give a good estimation of VG...but haven't seen anyone do it this way formally. I'll see if I can find some time to try it out.

### Adding deleterious mutations

From Kim:

you should be able to calculate the average fitness loss by your deleterious mutations from the mean effect size and mutation rate. I'll have to look in my notes when I get to school. Something along the lines of a fitness loss of e^-U where U is the genomic mutation rate. And so I think that times your fecundity should give you something like your overall rate of replacement, and just aim to get that around 1 or greater.   For example, we had a genomic mutation rate (for deleterious loci) of 1  (1000 del loci * mut rate of 0.001)
So e^-1 was ~0.36 and that times a fecundity of 3 put expected fitness >1.
On top of that though, we increased fecundity even more so that populations could persist at even lower fitness levels. This latter part could kind of be anything, and we set our fecundity based on how much RAM the runs ended up needing.

```{r}
# if we have 1000 deleterious alleles
exp(-(mu*1000))
exp(-(mu*5000))

B = function(U,M){exp(-U/M)}

B(50000*0.8*1e-06, 50*log(1/(1-2*1e-10))*50000)
```

background selection yields access of rare alleles, negative Tajima's D

gamma distribution with different mean and variance for each exon

